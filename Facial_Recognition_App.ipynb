{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRp23v7xlSME"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 opencv-python matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b2yNaBk2_GYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5JkyEfwcbNqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image, ImageFilter\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv2D, Layer,Dense, MaxPooling2D, Input, Flatten\n",
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "KDxjaHyrmVlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)"
      ],
      "metadata": {
        "id": "-L6QSb-Qobzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_path = os.path.join('data', 'positive')\n",
        "negative_path = os.path.join('data', 'negative')\n",
        "anchor_path = os.path.join('data', 'anchor')\n",
        "print(anchor_path)"
      ],
      "metadata": {
        "id": "kONbD8_BpUB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "POS_PATH"
      ],
      "metadata": {
        "id": "pwxMSgHWRNIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(positive_path)\n",
        "os.makedirs(negative_path)\n",
        "os.makedirs(anchor_path)"
      ],
      "metadata": {
        "id": "-iLgJtKaRUtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xf /content/drive/MyDrive/lfw.tgz"
      ],
      "metadata": {
        "id": "TC15U5L1Rkfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for directory in os.listdir('lfw'):\n",
        "  for file in os.listdir(os.path.join('lfw', directory)):\n",
        "    old_path = os.path.join('lfw', directory, file)\n",
        "    new_path = os.path.join(negative_path, file)\n",
        "    os.replace(old_path,new_path)\n"
      ],
      "metadata": {
        "id": "edHr_ImSgfpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for directory in os.listdir('data/negative'):\n",
        "  print(directory)"
      ],
      "metadata": {
        "id": "7gNtNN2ZkiPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Access Image Folders and convert them into tensorflow datasets\n",
        "anchor = tf.data.Dataset.list_files('/content/drive/MyDrive/data/anchor' + '/*.jpg').take(300)\n",
        "print(\"A\",anchor)\n",
        "positive = tf.data.Dataset.list_files('/content/drive/MyDrive/data/positive' + '/*.jpg').take(300)\n",
        "negative = tf.data.Dataset.list_files('/content/drive/MyDrive/data/negative' + '/*.jpg').take(300)\n",
        "\n",
        "\n",
        "dir_test = anchor.as_numpy_iterator()\n",
        "\n",
        "print(dir_test.next())\n"
      ],
      "metadata": {
        "id": "qq2rjYUxbSWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing step which includes scaling and resizing of images\n",
        "def preprocess_image(img_path):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    # Load in the image\n",
        "    img_object = tf.io.decode_jpeg(img)\n",
        "    print(\"I\",img_object)\n",
        "    grayscale_imgs = tf.image.rgb_to_grayscale([img_object])\n",
        "    grayscale_img = grayscale_imgs[0]\n",
        "    print(\"J\",grayscale_img)\n",
        "\n",
        "    # Preprocessing steps - resizing the image to be 100x100x3\n",
        "    grayscale_img = tf.image.resize(grayscale_img, (105,105))\n",
        "\n",
        "    #print(\"K\",grayscale_img)\n",
        "    # Scale image to be between 0 and 1\n",
        "    grayscale_img = grayscale_img / 255.0\n",
        "\n",
        "    # Return image\n",
        "    return grayscale_img\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bHjbsh5LiJYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = preprocess_image('/content/drive/MyDrive/data/positive/02b46c2b-2bd1-11ee-b6d4-5cbaefd5b04c.jpg')\n",
        "print(image)\n",
        "plt.imshow(image)\n"
      ],
      "metadata": {
        "id": "z_ozUv7eIavo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Constructing a labelled dataset\n",
        "negative_samples = tf.data.Dataset.zip((anchor, negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n",
        "positive_samples = tf.data.Dataset.zip((anchor, positive, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
        "data = positive_samples.concatenate(negative_samples)\n",
        "print(len(data))\n"
      ],
      "metadata": {
        "id": "qQaDLRvKjiUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting the dataset into training and testing datasets\n",
        "def preprocess_data(img1,img2,label):\n",
        "  return(preprocess_image(img1), preprocess_image(img2), label)\n",
        "#Preprocessing the entire dataset\n",
        "for i in data:\n",
        "  print(\"I\",i)\n",
        "data = data.map(preprocess_data)\n",
        "#Caching the dataset\n",
        "data = data.cache()\n",
        "#Shuffling the elements of the dataset\n",
        "data = data.shuffle(buffer_size=1024)\n",
        "samples = data.as_numpy_iterator()\n",
        "samp = samples.next()\n",
        "plt.imshow(samp[0])\n"
      ],
      "metadata": {
        "id": "_xbsTBnRnFYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(data))"
      ],
      "metadata": {
        "id": "bpGwQ6ghrD1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract training data\n",
        "train_data = data.take(round(len(data)*.7))\n",
        "train_data = train_data.batch(16)\n",
        "train_data = train_data.prefetch(8)\n",
        "\n",
        "#Extracting testing data\n",
        "test_data = data.skip(round(len(data)*.7))\n",
        "test_data = test_data.take(round(len(data)*.3))\n",
        "test_data = test_data.batch(16)\n",
        "test_data = test_data.prefetch(8)"
      ],
      "metadata": {
        "id": "fVTcyMjDsk5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(test_data))"
      ],
      "metadata": {
        "id": "-5QQUuf-EX6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the model which involves the creating the embedding layer follwed by creating the distance layer\n",
        "\n",
        "#Creating the embedding layer\n",
        "inp = Input(shape=(105,105,1), name='input_image')\n",
        "c1 = Conv2D(64, (10,10), activation='relu')(inp)\n",
        "m1 = MaxPooling2D(64, (2,2), padding='same')(c1)\n",
        "c2 = Conv2D(128, (7,7), activation='relu')(m1)\n",
        "m2 = MaxPooling2D(64, (2,2), padding='same')(c2)\n",
        "c3 = Conv2D(128, (4,4), activation='relu')(m2)\n",
        "m3 = MaxPooling2D(64, (2,2), padding='same')(c3)\n",
        "c4 = Conv2D(256, (4,4), activation='relu')(m3)\n",
        "f1 = Flatten()(c4)\n",
        "d1 = Dense(4096, activation='sigmoid')(f1)\n",
        "embedding_layer = Model(inputs=[inp], outputs=[d1], name='embedding_layer')\n",
        "embedding_layer.summary()"
      ],
      "metadata": {
        "id": "8pkcFz2ZGS42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating the distance layer\n",
        "def L1distLayer(input1,input2):\n",
        "  return tf.math.abs(input1 - input2)"
      ],
      "metadata": {
        "id": "Lj80xDZWI4fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aYqzht1uHiAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Constructing the final Siamese Model\n",
        "input_img = Input(name='input_img', shape=(105,105))\n",
        "ver_img = Input(name='verification_img', shape=(105,105))\n",
        "inp_embedding = embedding_layer(input_img )\n",
        "ver_embedding = embedding_layer(ver_img)\n",
        "\n",
        "\n",
        "distances = L1distLayer(inp_embedding, ver_embedding)\n",
        "final_classifier = Dense(1, activation='sigmoid')(distances)\n",
        "final_network = Model(inputs=[input_img, ver_img], outputs=final_classifier, name='SiameseNetwork')\n",
        "final_network.summary()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B4JfDpujMjyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_network.summary()"
      ],
      "metadata": {
        "id": "QCSLZ2CWXoF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Choose a loss function(Binary Cross Entropy in this case)\n",
        "loss = tf.losses.BinaryCrossentropy()\n",
        "\n",
        "#Choose an optimizer(Adam in this case)\n",
        "optimizer = tf.keras.optimizers.Adam(1e-4)   #Learning rate = 1e-4\n"
      ],
      "metadata": {
        "id": "_w9-G5JtXr9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create checkpoints\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
        "checkpoint = tf.train.Checkpoint(opt=optimizer, siamese_model= final_network)"
      ],
      "metadata": {
        "id": "rDsly5C1Zpyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementing the training function\n",
        "\n",
        "def train(batch):\n",
        "\n",
        "    # Record all of our operations\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Get anchor and positive/negative image\n",
        "        x = batch[:2]\n",
        "        # Get label\n",
        "        y = batch[2]\n",
        "\n",
        "        # Forward pass\n",
        "        y_cal = final_network(x, training=True)\n",
        "        # Calculate loss\n",
        "        b_loss = loss(y, y_cal)\n",
        "    print(b_loss,type(b_loss))\n",
        "\n",
        "    # Calculate gradients\n",
        "    print(\"L\")\n",
        "    grad = tape.gradient(b_loss, final_network.trainable_variables)\n",
        "    print(\"K\")\n",
        "\n",
        "    # Calculate updated weights and apply to siamese model\n",
        "    optimizer.apply_gradients(zip(grad, final_network.trainable_variables))\n",
        "\n",
        "    # Return loss\n",
        "    return b_loss"
      ],
      "metadata": {
        "id": "PoOKAr7WaCRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementing the training loop\n",
        "def train_model(data, epochs):\n",
        "    # Loop through epochs\n",
        "    for epoch in range(1, epochs+1):\n",
        "        print('\\n Epoch {}/{}'.format(epoch, epochs))\n",
        "        progress_bar = tf.keras.utils.Progbar(len(data))\n",
        "\n",
        "        # Loop through each batch\n",
        "        for idx, batch in enumerate(data):\n",
        "            # Run train step here\n",
        "            train(batch)\n",
        "            progress_bar.update(idx+1)\n",
        "\n",
        "        # Save checkpoints\n",
        "        if epoch % 2 == 0:\n",
        "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "#Training the model\n",
        "train_model(train_data, 15)"
      ],
      "metadata": {
        "id": "xCHA3b4NcGTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"a\")"
      ],
      "metadata": {
        "id": "fkxalr7eDxEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"B\")"
      ],
      "metadata": {
        "id": "Pg8hxdeJceit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"N\")"
      ],
      "metadata": {
        "id": "igcQhDDJGKoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input, test_val, y_true = test_data.as_numpy_iterator().next()\n",
        "y_hat = final_network.predict([test_input, test_val])\n",
        "y_hat"
      ],
      "metadata": {
        "id": "ZZsmh_9HIuWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "m = Recall()\n",
        "\n",
        "# Calculating the recall value\n",
        "m.update_state(y_true, y_hat)\n",
        "\n",
        "# Return Recall Result\n",
        "m.result().numpy()"
      ],
      "metadata": {
        "id": "HB2jYodDJ6ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_network.save('siamesemodel.h5')"
      ],
      "metadata": {
        "id": "qcTDYNXJKITY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_model = tf.keras.models.load_model('/content/drive/MyDrive/siameseNetwork.h5',\n",
        "                                   custom_objects={'L1Dist': L1distLayer, 'BinaryCrossentropy':tf.losses.BinaryCrossentropy})\n",
        "siamese_model.summary()"
      ],
      "metadata": {
        "id": "5zJfM-va_I8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Verification using Real Time Dataset\n",
        "input_img = preprocess_image('/content/drive/MyDrive/IMG_20230212_160419.jpg')\n",
        "anchor_img = preprocess_image('')\n",
        "result = siamese_model.predict(list(np.expand_dims([input_img, anchor_img], axis=1)))\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "mWNomNwNKuSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uuWLRQ-gdIcf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}